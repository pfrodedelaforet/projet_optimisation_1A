{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation\n",
    "\n",
    "Ecrivons la relation entre les variations de prix et de quantité produites : \n",
    "$x_{g}$ est la quantité de gouda vendue, $\\epsilon_{g}$ l'élasticité prix associée.\n",
    "\n",
    "$x_{e}$ est la quantité d'edam vendue, $\\epsilon_{e}$ l'élasticité prix associée.\n",
    "\n",
    "$x_{b}$ est la quantité de beurre vendue, $\\epsilon_{b}$ l'élasticité prix associée.\n",
    "\n",
    "$x_{l}$ est la quantité de lait vendue, $\\epsilon_{l}$ l'élasticité prix associée. \n",
    "\n",
    "On introduit également l'élasticité croisée de g par rapport à e$\\epsilon_{g/e}=\\frac{\\frac{dx_g}{x_g}}{\\frac{dp_e}{p_e}}$, et celle de e par rapport à g : $\\epsilon_{e/g}=\\frac{\\frac{dx_e}{x_e}}{\\frac{dp_g}{p_g}}$\n",
    "\n",
    "On a: $dx_g = \\epsilon_{g} * \\frac{dp_g * x_g}{p_g} + \\epsilon_{g/e} * \\frac{dp_e * x_g}{p_e}$, $dx_e = \\epsilon_{e} * \\frac{dp_e * x_e}{p_e} + \\epsilon_{e/g} * \\frac{dp_g * x_e}{p_g}$, $dx_b = \\epsilon_{b} * \\frac{dp_b * x_b}{p_b}$, $dx_l = \\epsilon_{l} * \\frac{dp_l * x_l}{p_l}$. Ici, les différentielles correspondront concrètement à l'écart entre l'année N et l'année N+1. \n",
    "\n",
    "On a également (Q2), en notant $G_y$ la teneur en matière grasse du produit commencant par la lettre y, \n",
    "$x_g*G_g+x_l*G_l+x_b*G_b+x_e*G_e \\le M $(C1) où M est la quantité totale de matière grasse dans le lait brut.\n",
    "\n",
    "On a d'autre part, (Q3), en notant $L_y$ la teneur en lactose du produit commencant par la lettre y : \n",
    "$x_g*L_g+x_l*L_l+x_b*L_b+x_e*L_e \\le L_{laitbrut}*x_{laitbrut} $(C2).\n",
    "\n",
    "On a une troisième condition : Pour garantir la paix sociale : la moyenne-pondérée par la part dans le budget du consommateur du produit- des variations relatives de prix doit être négative, ie : $\\frac{dp_{g}}{p_g}*\\frac{x_g*p_g}{budget} +\\frac{dp_{e}}{p_e}*\\frac{x_e*p_e}{budget}+ \\frac{dp_{b}}{p_b}*\\frac{x_b*p_b}{budget}+\\frac{dp_{l}}{p_l}*\\frac{x_l*p_l}{budget}\\le 0$, autrement dit : $dp_{g}*\\frac{x_g}{budget} +dp_{e}*\\frac{x_e}{budget}+ dp_{b}*\\frac{x_b}{budget}+dp_{l}*\\frac{x_l}{budget}\\le 0 $(C3). Ici aussi, les différentielles correspondront concrètement à l'écart entre l'année N et l'année N+1. \n",
    "\n",
    "On pose\n",
    "\n",
    "$\\begin{array}{ccccc}\n",
    "f &:&R^{4}& \\to& R\\\\\n",
    "&&(p_g, p_e, p_b, p_l)&\\mapsto&x_g*p_g(x_g, x_e)+x_e*p_e(x_g, x_e)+x_b*p_b(x_b)+x_l*p_l(x_l) \\\\\n",
    "\\end{array}$. \n",
    "Evidemment, les quantités à vendre doivent être positives, pour que l'énoncé ait un sens.\n",
    "Pour définir la fonction f sur un ouvert (l'ensemble des réels), on choisit de rajouter des contraintes de positivité sur les 4 quantités. \n",
    "\n",
    "$-x_g(p_g, p_e) < 0$ (C4)\n",
    "\n",
    "$-x_e(p_g, p_e) < 0$ (C5)\n",
    "\n",
    "$-x_b(p_b) < 0$ (C6)\n",
    "\n",
    "$-x_l(p_l) < 0$(C7).\n",
    "\n",
    "On cherche donc à minimiser $-f$ sous les 7 contraintes ci-dessus. Notons qu'on a choisi d'exprimer $f$ en fonction des prix, car les élasticités sont supposées constantes, donc on peut relier par une relation linéaire prix et quantité. Etudier une fonction des prix ou des quantités revient à la même chose en terme d'optimisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse du problème\n",
    "Etudions la fonction $g=-f$ à minimiser. \n",
    "Après implémentation des valeurs numériques des différents paramètres, on a : en notant 1 l'indice du lait, 2 l'indice du beurre, 3 l'indice du gouda, 4 l'indice de l'edam : \n",
    "x1 = −1.5413 * p1 + 2671\n",
    "\n",
    "x2 = −0.0203 * p2 + 135,\n",
    "\n",
    "x3 = −0.0136 * p3 + 0.0015 * p4 + 103,\n",
    "\n",
    "x4 = 0.0016 * p3 − 0.0027 * p4 + 19\n",
    "\n",
    "0.026 * x1 + 0.800 * x2 + 0.306 * x3 + 0.241 * x4 ≤ 121, Matière grasse\n",
    "\n",
    "0.086 * x1 + 0.020 * x2 + 0.297 * x3 + 0.371 * x4 ≤ 250, Lactose\n",
    "\n",
    "0.0160 * x1 + 0.0004 * x2 + 0.0005 * x3 + 0.0002 * x4 ≤ 10. Social\n",
    "\n",
    "Finalement, en remplaçant les $x_{i}$ par leur expression en fonction de $p_{i}$ dans la fonction, on obtient g(p) = $-b^{t}*p + \\frac{1}{2} * p^{t} * A * p$ avec A et B connus. Les conditions étant linéaire, elles peuvent être résumées dans la condition $C*p \\le d$.\n",
    "\n",
    "Identifions la fonction g : le hessien de g vaut $\\frac{A^{t}+A}{2}=A$ car A est symétrique. Etudions l'éventuelle positivité de A : en calculant son polynôme caractéristique, on obtient : $(X-3.0825)(X-0.0405)(X^{2}-0.0811X+0.00145379)$. La somme et le produit des racines du troisième facteur sont tous les deux positifs, ces racines sont donc positives. Il s'agit donc d'une matrice définie positive (KerA est l'ensemble vide). On a donc que le hessien de g est défini positif, donc g est strictement convexe, et même alpha-convexe : on a que le hessien de g est constant et strictement positif, donc avec alpha = inf{vap(A)}>0, on a bien que le hessien de g est supérieur ou égal à alpha*Id (leur différence est définie positive). Au niveau du conditionnement, on a un inégalité sur un système linéaire. Donc la fonction qui à p associe c(p) est convexe.C est une matrice de taille (7, 4). On a donc à minimiser une fonction convexe sur un ensemble convexe(car c est linéaire), donc ce problème de minimisation admet une unique solution. Au vu de la forme quadratique de notre fonction à minimiser, ainsi que de la linéarité des contraintes, l'algorithme qui paraît le plus adapté est l'algorithme des contraintes actives QP, qui converge en un nombre fini d'itérations vers l'unique optimum global du système. Cependant, cet algorithme nécessite d'utiliser des fonctions de minimisation avec contrainte égalité de scipy, ce qui est dommage, donc on préferera utiliser l'algorithme d'Arrow Hurwicz pour le calcul des coûts optimaux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "C = np.array([[-0.0401, -0.0162, -0.0039, 0.0002],\n",
    "              [-0.1326, -0.0004, -0.0034, 0.0006], \n",
    "              [1.5413, 0, 0, 0], \n",
    "              [0, 0.0203, 0, 0], \n",
    "              [0, 0, 0.0136, -0.0015], \n",
    "              [0, 0, -0.0016, 0.0027], \n",
    "              [0.0160, 0.0004, 0.0005, 0.0002]])\n",
    "A = np.zeros((4, 4))\n",
    "A[0][0] = 3.0825 ;A[1][1] = 0.0405 ; A[2][2] = 0.0271 ; A[3][3] = 0.0054 ; A[2][3] = -0.0031 ; A[3][2] = -0.0031 ;\n",
    "A\n",
    "b = np.array([-2671, -135, -103, -19])#on prend -b pour  avoir la forme quadratique classique\n",
    "d = np.array([-92.6, -29.0, 2671, 135, 103, 19, 10])\n",
    "def QP(A, b, C, d, w_0, x_0):\n",
    "    pas_fini = True\n",
    "    w = w_0 ; x_k = x_0\n",
    "    while pas_fini : #on regarde KKT : -grad g = ? sum(lambda i ci)\n",
    "        grad_g = b - A.dot(x_k)\n",
    "        C_w = np.zeros((len(w), 4))\n",
    "        for i in range(len(w)) : \n",
    "            C_w[i] = np.copy(C[w[i]])#C_w est construit pour forcer à être CL des contraintes actives uniquement\n",
    "        lambds = np.linalg.solve(np.transpose(C_w), -grad_g)\n",
    "        if all(lambds) > 0 : \n",
    "            pas_fini = False\n",
    "        else :\n",
    "            #on résout min\n",
    "            def constrain(x):\n",
    "                return C_w.dot(x)\n",
    "            def f_p(p):\n",
    "                return 0.5 * np.transpose(p).dot(A).dot(p) + np.transpose(A.dot(x_k)+d).dot(p)\n",
    "            pk = minimize(f_p, 0, method = \"SLSQP\", constraints = {\"fun\" : constrain, \"type\" : \"eq\"}).x\n",
    "            if pk != 0:\n",
    "                w_c = list(set([i for i in range(len(C))])-set(w))\n",
    "                for i in range(w_c):#sur i n'appartenant pas à l'espace de travail\n",
    "                    if np.transpose(C[i]).dot(pk)<= 0:\n",
    "                        w_c.pop(i)\n",
    "                boue = [(d[i]-np.transpose(C[i]).dot(x_k))/(np.transpose(C[i]).dot(pk)) for i in w_c]\n",
    "                m = min(boue)\n",
    "                alphak = min(1, m)\n",
    "                if alphak < 1 : \n",
    "                    j = argmin(boue)\n",
    "                    w.append(j)\n",
    "    return x_k\n",
    "\n",
    "\n",
    "print(QP(A, b, C, d, [], np.zeros(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_lag(A, b, C, d, x, lambd):\n",
    "    grad_x = A.dot(x)+b\n",
    "    return np.sqrt(np.linalg.norm(A.dot(x)+b)**2+np.linalg.norm(C.dot(x)-d)**2)\n",
    "\n",
    "def optim_gradient_fixed_step(grad_fun, x0, l, max_iter = 100000, epsilon_grad_fun = 1e-8):\n",
    "    # To complete\n",
    "    i = 0\n",
    "    gradi = grad_fun(x0)\n",
    "    xk = x0\n",
    "    while (i < max_iter and np.linalg.norm(gradi) > epsilon_grad_fun): \n",
    "        gradi = grad_fun(xk)\n",
    "        xk -= l * gradi\n",
    "        i += 1\n",
    "    return xk, i\n",
    "\n",
    "\n",
    "def Arrow(A, b, C, d, x0, lambd0, alpha, eps, max_iter, norm_lim):\n",
    "    xk = x0 ; lambdk = lambd0 ; k = 0\n",
    "    def lag(x):\n",
    "        return 0.5*np.transpose(x).dot(A).dot(x) + np.transpose(b).dot(x) +np.transpose(lambdk).dot(C.dot(x)-d)\n",
    "    def grad_lag(x):\n",
    "        return A.dot(x)+b+np.transpose(C).dot(lambdk)\n",
    "    while k < max_iter:\n",
    "        xk -= grad_lag(xk)*eps\n",
    "        lambdk += alpha * (C.dot(xk)-d)\n",
    "        for l in range(len(lambdk)) : \n",
    "            if lambdk[l] < 0 :\n",
    "                lambdk[l] = 0\n",
    "        k += 1\n",
    "    return (xk, k)\n",
    "            \n",
    "x = Arrow(A, b, C, d, np.ones(4), np.zeros(7), 0.1, 0.1, 1500000, 0.01)  \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_lag(x):\n",
    "        return A.dot(x)+b\n",
    "e_1000000 = np.array([ 420.34504894, 4024.64289728, 2775.35063497, 1395.22825295])\n",
    "e_1_500_000 = np.array([ 420.23444463, 4025.00227769, 2774.96412957, 1394.00155788])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.dot(e_1_500_000)-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après utilisation de l'algorithme d'Arrow-Hurwicz, on obtient une convergence vers p =[ 420.23444463, 4025.00227769, 2774.96412957, 1394.00155788]. On observe également que les multiplicateurs de Lagrange des contraintes 1 et 7 sont nuls, ce qui signifie que ces contraintes sont les deux seules contraintes à ne pas être actives. La solution x* du problème est telle que pour tout i $\\in \\Lambda$ l'ensemble des indices des contraintes actives, on ait ci(x*) = 0, soit$(Cx^{*})_i = d_i$. Ici, les contraintes actives sont les contraintes 2, 3, 4, 5, et 6. Du coup, \n",
    "\n",
    "$\\begin{pmatrix}-0.1326&-0.0004&-0.0034&0.0006\\\\\n",
    "1.5413&0&0&0\\\\\n",
    "0&0.0203&0&0\\\\\n",
    "0&0&0.0136&-0.0015\\\\\n",
    "0&0&-0.0016&0.0027\\\\\n",
    "\\end{pmatrix}$ \n",
    "$\\begin{pmatrix}p_1\\\\\n",
    "p_2\\\\\n",
    "p_3\\\\\n",
    "p_4\\\\\n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}-29.0\\\\\n",
    "2671\\\\\n",
    "135\\\\\n",
    "103\\\\\n",
    "19\\\\\n",
    "\\end{pmatrix}$\n",
    "On considère une perturbation de l'élasticité-prix de chacun des produits. Si on note x* la solution de notre problème non perturbé et $\\lambda$* le multiplicateur de Lagrange associé, x* vérifie l'équation matricielle au dessus. Si on perturbe les élasticités, en calculant Cix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
